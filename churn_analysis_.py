# -*- coding: utf-8 -*-
"""Churning_analysis_P.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tswhzzGTtTKazNPOQtKQIrZC8hARcJib
"""

import pandas as pd
import numpy as np

df=pd.read_csv("/content/drive/MyDrive/Churn Analysis/churn.csv")

df

df.head()

df.drop("customer_id",axis=1,inplace=True)        #dropping customer id as it is not that informativefor analysis

df

df.info()       #structure of dataframe

from sklearn.preprocessing import LabelEncoder                        # #using LabelEncoder to encode categorical data in df
label_encoder = LabelEncoder()                                                        #initializing object
df['country'] = label_encoder.fit_transform(df['country'])          #it fits the label encoder to unique values of country and gender column  and transform the country and gender column into integer values in DataFrame
df['gender'] = label_encoder.fit_transform(df['gender'])
df

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()                                                     #mixmax object initailization
df_normalized = scaler.fit_transform(df)                      #calculates the minimum and maximum values of each feature and then scales the values accordingly

df_normalized                                                                       #df_normalized will be a new DataFrame with the same structure as your original DataFrame, but the numerical values will be scaled between 0 and 1.

#splitting your DataFrame into feature variables (X) and the target variable (y)
X = df.drop('churn', axis=1)                                              #X will contain all the columns from df except the 'churn' column.  Dropping churn column from og df
y = df['churn']                                                                        #churn is output/target variable. Y contains the churn column values which needs to be predicted as churned or not churned 1and 0 respectively

# split dataset into training and testing sets
from sklearn.model_selection import train_test_split                                              #train_test_split specifies the proportion of the dataset that should be used for testing here 20%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)             #random_state to get same train-test split if we run the code multiple times for random number generation

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming X_train, X_test, y_train, y_test are already defined
# using train_test_split as explained in the previous responses

# Create a Decision Tree model
dt_classifier = DecisionTreeClassifier(random_state=42)                                                     #initializes a Decision Tree classifier with a specified random state for reproducibility.

# Train the model
dt_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred_dt = dt_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred_dt)                                                                  #finding accuracy of model and omparing the predicted labels (y_pred_dt) with the actual labels from the test set (y_test)
print(f"Decision Tree Accuracy: {accuracy}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Assuming X_train, X_test, y_train, y_test are already defined

# Create a Random Forest model
rf_classifier = RandomForestClassifier(random_state=42)

# Train the model
rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred_rf = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {accuracy}")

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

# Assuming X_train, X_test, y_train, y_test are already defined

# Standardize the features (scaling is often important for SVM)
scaler = StandardScaler()                                                                              #The features (X_train and X_test) are standardized using StandardScaler to have zero mean and unit variance as sensitive to the scale of input features.
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create an SVM model
svm_classifier = SVC(random_state=42)

# Train the model
svm_classifier.fit(X_train_scaled, y_train)

# Make predictions on the scaled test set
y_pred_svm = svm_classifier.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred_svm)
print(f"SVM Accuracy: {accuracy}")

#genrating confusion matrices formultiple classifiers(SVM, Random Forest, Decision Tree) and printing on loop
from sklearn.metrics import confusion_matrix
l1=[y_pred_svm,y_pred_rf,y_pred_dt]                                       #predicted labels from different classifiers.
for i in l1:
  conf_matrix = confusion_matrix(y_test, i)
  print("Confusion Matrix:")
  print(conf_matrix)

from sklearn.metrics import precision_score, recall_score, f1_score
for y_pred in l1:
  precision = precision_score(y_test, y_pred)
  recall = recall_score(y_test, y_pred)
  f1 = f1_score(y_test, y_pred)

  print(f"Precision: {precision}")
  print(f"Recall: {recall}")
  print(f"F1 Score: {f1}")

from sklearn.metrics import classification_report
for y_pred in l1:
  report = classification_report(y_test, y_pred)
  print("Classification Report:")
  print(report)

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
for  predicted_probabilities in l1:

# Assuming binary classification
  fpr, tpr, thresholds = roc_curve(y_test, predicted_probabilities)
  roc_auc = roc_auc_score(y_test, predicted_probabilities)

  plt.plot(fpr, tpr, label=f'AUC = {roc_auc}')
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.legend()
  plt.show()

'''

H.W (any 2 hw on/before saturday)
reason behind every step and descions made, explain steps.   /////
implement : min max scaling , report outcomes
implement : try implement other algorithms (naive bayes, knn etc).

'''

#Naive Bais
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Assuming you have a dataset X (features) and y (labels)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Naive Bayes classifier
nb_classifier = GaussianNB()

# Train the classifier
nb_classifier.fit(X_train, y_train)

# Make predictions
y_pred_nb = nb_classifier.predict(X_test)

# Evaluate the accuracy
accuracy = accuracy_score(y_test, y_pred_nb)
print(f'Accuracy: {accuracy}')

#KNN
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Assuming you have a dataset X (features) and y (labels)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a KNN classifier (let's say, k=3)
knn_classifier = KNeighborsClassifier(n_neighbors=3)

# Train the classifier
knn_classifier.fit(X_train, y_train)

# Make predictions
y_pred_knn = knn_classifier.predict(X_test)

# Evaluate the accuracy
accuracy = accuracy_score(y_test, y_pred_knn)
print(f'Accuracy: {accuracy}')

from sklearn.preprocessing import MinMaxScaler
# Create a MinMaxScaler object
scaler = MinMaxScaler()
# Fit the scaler on your data and transform it
scaled_data = scaler.fit_transform(df)

# The scaled_data variable now contains the MinMax-scaled values
print("Original Data:\n", df)
print("\nScaled Data:\n", scaled_data)

scaled_data

column_index_of_churn = 10  # Replace with the actual index of 'churn'
X = np.delete(scaled_data, column_index_of_churn, axis=1)
y = scaled_data[:, column_index_of_churn].astype('int')

df.info()

from sklearn.metrics import confusion_matrix
l1=[y_pred_knn,y_pred_nb]
for i in l1:
  conf_matrix1 = confusion_matrix(y_test, i)
  print("Confusion Matrix:")
  print(conf_matrix1)

from sklearn.metrics import precision_score, recall_score, f1_score
for y_pred in l1:
  precision1 = precision_score(y_test, y_pred)
  recall1= recall_score(y_test, y_pred)
  f2 = f1_score(y_test, y_pred)

  print(f"Precision: {precision1}")
  print(f"Recall: {recall1}")
  print(f"F1 Score: {f2}")

